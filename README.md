# Disk Monitoring Solution

## Overview

This project delivers a scalable, secure, and efficient disk utilization monitoring solution tailored for enterprises managing virtual machines across multiple cloud providers — specifically AWS, Azure, and GCP.

As companies grow and onboard numerous cloud accounts and projects, monitoring disk space becomes critical to prevent service disruptions caused by disk exhaustion. Leveraging Ansible for configuration management and prioritizing cloud-native services only when they add significant value, this solution provides:

- Centralized and secure management of VMs across diverse cloud accounts and environments.
- Automated collection and aggregation of disk usage metrics from all VMs.
- Forecasting of disk utilization trends using machine learning algorithms to proactively predict low disk space conditions and prevent downtime.
- Scalable architecture designed to accommodate continuous growth in the number of VMs and cloud accounts.
- Extensibility and maintainability through modular Ansible playbooks and infrastructure as code.

> **Note:** For demonstration purposes, some credentials, quotes, and security permissions in the code and configuration are  simplified. In real production environments, these values must be securely injected using environment variables, secrets management tools, or dynamic configuration to ensure security and compliance.

---

## Technology Stack

- Python
- AWS Cloud Services: Lambda, SNS, CloudWatch, IAM Roles & Policies, SSM, AWS Glue, EventBridge, EC2, S3 Storage, AWS Secrets Manager
- Ansible for automation and configuration management

---

---

## Folder structure 

To maintain clarity, modularity, and scalability, the project is organized with clear separation of concerns across multiple directories:

- **`ansible/`**  
  Contains all Ansible automation artifacts including playbooks, inventories, and static files. This folder orchestrates disk usage collection and system setup across multi-cloud environments.

- **`ansible/files/`**  
  Stores scripts and binaries that are copied verbatim to target hosts during playbook runs, such as Linux shell scripts and Windows PowerShell scripts for disk monitoring and client setup.

- **`scripts/`**  
  Holds auxiliary helper scripts used locally for environment configuration, cloud firewall setup, and network simulation.

- **`iam_policies/` and `cloud_rules/`**  
  Contain JSON definitions for AWS IAM policies and cloud provider firewall/security group rules essential for secure, compliant automation access.

- **`storage_reports/`**  
  Serves as the output directory for all disk usage reports and aggregated metrics generated by playbooks, organized per client for easy review.

- **Root-level files** such as `README.md`, architecture diagrams, and `.gitignore` provide project documentation, visual context, and repository hygiene respectively.

---

---

## Deployment Overview

We are using **Ansible** as the automation tool to collect comprehensive storage-related information from client machines. The Ansible control node is intended to be deployed on an **AWS EC2 instance running Linux**. For demonstration purposes, however, the solution is executed locally on a Linux-based virtual machine using **dummy IP addresses** to simulate client connectivity. Please refer to the provided **client list with IP addresses CSV** for details.

This demonstration showcases the core functionality of the solution. In a real-world deployment, the Ansible control server would be hosted on AWS cloud infrastructure (Linux EC2 instances), with proper network configurations and security groups in place.

Throughout the documentation, security groups and related AWS cloud components are explained with the assumption that the solution is deployed in an AWS environment, even though the demo runs locally.

---

## Secure Connectivity to Client Machines

To securely connect to client machines, different protocols are used depending on the operating system:

- For **Linux-based systems**, we use **SSH (Secure Shell)** to establish encrypted connections.
- For **Windows-based systems**, we use **WinRM (Windows Remote Management)**, which is Microsoft's implementation of the WS-Management protocol.

In this demonstration, running locally with dummy IP addresses, we simulate secure SSH connections using **SSH key pairs**:

- Each SSH key pair consists of a **public key** and a **private key**.
- The **public key** is installed on the client machine.
- The **private key** remains securely on the Ansible control node.
- When a connection attempt is made, the keys are matched to authenticate access.

Connectivity is further controlled via **security group rules** which specify allowed inbound and outbound traffic, ensuring that only authorized connections between the control node and clients are permitted.

For demonstration purposes, we simulate these connections on a local virtual machine environment with dummy IP addresses to illustrate communication flows.

Please refer to the relevant **security group configurations and IAM policies** included in this project:

- **Security Group Rules:** Define network-level permissions to allow SSH or WinRM traffic.
- **IAM Policies:** Define the specific AWS permissions assigned to roles, which represent policies in action.

This setup demonstrates how secure, controlled connectivity is maintained between the control node and client machines in a real-world, multi-cloud deployment.


## Adding Dummy IP Addresses

To simulate multiple client machines locally, dummy IP addresses are added to the control machine’s network interfaces. Please refer to the script `add_ip_aliases.sh` located in the `scripts/` folder for this setup.

Note :- commands written for the Unix/Linux shell (like bash, sh, zsh).

## SSH Connectivity and Host Key Management

To securely connect to client machines running Linux, we use **SSH (Secure Shell)** with key-based authentication, which ensures encrypted and passwordless access.

### Generating SSH Keys

- Use `ssh-keygen` to create a public/private SSH key pair on your control machine.  
- The **public key** is placed on each client machine in the `~/.ssh/authorized_keys` file.  
- The **private key** remains securely on the control machine and is used to authenticate connections.

### Using Dummy IP Addresses Locally

- For demonstration, dummy IP addresses are added to the control machine using the script `add_ip_aliases.sh` located in the `scripts/` folder.  
- This simulates multiple client VMs, allowing you to test SSH connectivity as if connecting to separate hosts.

### Automatically Trusting SSH Host Keys

- SSH requires verifying each host’s key on first connection.  
- The script `accept_all_ssh_keys.sh` (also in `scripts/`) automates fetching and adding all dummy hosts’ SSH keys to the local `known_hosts` file.  
- This prevents manual confirmation prompts during automation runs.

### Configuring Ansible

- In your Ansible inventory, specify the dummy IPs and configure the SSH user and private key path for authentication.  
- This setup enables Ansible to connect securely and automatically to all client hosts.

Using this approach, the control machine can establish **secure, passwordless SSH connections** to all simulated client machines, ensuring smooth and reliable automation workflows.

refer to inventory.yml file and client_list_with_ip.csv for details.



## Data Collection, Processing, and Notification Workflow

Once a secure connection with the client machine is established, Ansible refers to the inventory file to retrieve client details such as IP addresses and authentication credentials. Ansible then executes playbooks that run scripts on the client machines to collect system storage information, which is stored locally on the control node.

The collected data is processed using **AWS Glue**, which cleans and transforms the data for further analysis. To notify clients when their storage utilization crosses critical thresholds, the system leverages **AWS SNS** (Simple Notification Service) for real-time alerts.

Additionally, the solution implements a predictive notification mechanism based on time series analysis. Historical storage data from the past two months is collected and used to train an ML model using the **Prophet algorithm**. This model forecasts storage usage for the next 15 days, enabling proactive notifications before the storage reaches full capacity.

The time series data is first extracted from client VMs, then ingested and transformed via AWS Glue into a format suitable for training. The trained model provides forecasts that drive notification workflows, helping clients manage storage efficiently and avoid downtime.

Please refer to the storage reports generated for detailed insights into the storage metrics and forecasting process.



## Architecture diagram

![Disk Monitoring Architecture](./ARCHITECTURE%20DIAGRAM.jpg)
